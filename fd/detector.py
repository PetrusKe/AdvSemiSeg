import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

import cv2

""" Flaw Detector for pixel-level semi-supervised learning.

Flaw Detector is a neural network module proposed by us. 
It is used in pixelssl.ssl_framework.ds_framework and pixelssl.ssl_framework.adv_framework.

It takes (image, prediction) pair as the input and output a flaw map 
which is activated on the prediction's flaw regions. 
The ground truth of Flaw Detector (fd_gt) is generated by (prediction, gt) pair.
fd_gt is generated by different ways for different tasks.

This file includes the structure of Flaw Detector and the criterion to train it.
"""


def generate_flaw_detector_gt(pred, gt, num_classes=21, ignore_index=255):
        pred = pred.detach()

        # expanding the gt to match the pred
        one_hot = torch.zeros(pred.shape).cuda()
        for i in range(0, num_classes):
            one_hot[:, i:i+1, ...][gt == i] = 1
        
            # ignore the segment boundary
            one_hot[:, i:i+1, ...][gt == ignore_index] = -1
            pred[:, i:i+1, ...][gt == ignore_index] = -1

        diff = torch.sum((pred - one_hot) ** 2, dim=1, keepdim=True)
        pred_idx = torch.max(pred, dim=1, keepdim=True)[1]
        diff[pred_idx.long() == gt.long()] = 0
        
        detect_gt = diff.data.cpu().numpy() * 255.0

        clip_threshold = 30
        blur_kernel = int(diff.shape[-1] / 8.0)
        blur_kernel = blur_kernel + 1 if blur_kernel % 2 == 0 else blur_kernel
        reblur_kernel = int(diff.shape[-1] / 4.0)
        reblur_kernel = reblur_kernel + 1 if reblur_kernel % 2 == 0 else reblur_kernel

        for sdx in range(0, detect_gt.shape[0]):
            handled = detect_gt[sdx, 0, ...]

            handled = cv2.GaussianBlur(handled, (blur_kernel, blur_kernel), 0)
            handled = cv2.threshold(handled, clip_threshold, 255, cv2.THRESH_TOZERO)[1]

            handled = cv2.dilate(handled, None, iterations=2)

            handled = cv2.GaussianBlur(handled, (reblur_kernel, reblur_kernel), 0)
            finished = (handled - handled.min()) / (handled.max() - handled.min() + 1e-6)
            detect_gt[sdx, 0, ...] = finished

        return torch.FloatTensor(detect_gt).cuda()

class MinimumCriterion(nn.Module):
    def __init__(self):
        super(MinimumCriterion, self).__init__()
        self.name = 'minimum'

    def forward(self, pred):
        pred = torch.pow(pred, 2)
        loss = torch.mean(pred)
        return loss


class FlawDetectorCriterion(nn.Module):
    def __init__(self):
        super(FlawDetectorCriterion, self).__init__()
        self.abs_criterion = AbsoluteCriterion()
        self.grad_criterion = GradientCriterion()

    def forward(self, pred, gt):
        abs_loss = self.abs_criterion.forward(pred, gt)
        grad_loss = 10.0 * self.grad_criterion.forward(pred, gt)
        detect_loss = abs_loss + grad_loss

        return torch.mean(detect_loss)


class FlawDetector(nn.Module):
    def __init__(self, in_channels):
        super(FlawDetector, self).__init__()

        self.enc1 = nn.Sequential(
            nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
        )
        self.enc2 = EncoderBlock(16, 32, stride=2)
        self.enc3 = EncoderBlock(32, 64, stride=2)
        self.enc4 = EncoderBlock(64, 128, stride=2)
        # self.enc5 = EncoderBlock(128, 256, stride=2)

        # self.dec0 = DecoderBlock(256 + 128, 128)
        self.dec1 = DecoderBlock(128 + 64, 64)
        self.dec2 = DecoderBlock(64 + 32, 32)
        self.dec3 = DecoderBlock(32 + 16, 16)
        self.final = nn.Conv2d(16, 1, 1)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                self.init_conv(m)
            elif isinstance(m, nn.BatchNorm2d):
                self.init_norm(m)

    def forward(self, task_inp, task_pred):
        x = torch.cat((task_inp, task_pred), 1)
        enc1 = self.enc1(x)
        enc2 = self.enc2(enc1)
        enc3 = self.enc3(enc2)
        enc4 = self.enc4(enc3)
        # enc5 = self.enc5(enc4)

        # dec0 = self.dec0(enc5, enc4)
        # dec1 = self.dec1(dec0, enc3)
        dec1 = self.dec1(enc4, enc3)
        dec2 = self.dec2(dec1, enc2)
        dec3 = self.dec3(dec2, enc1)
        final = self.final(dec3)

        x = torch.sigmoid(final)
        return x

    def init_conv(self, conv):
        nn.init.kaiming_uniform_(conv.weight, a=0, mode='fan_in', nonlinearity='relu')
        if conv.bias is not None:
            nn.init.constant_(conv.bias, 0)

    def init_norm(self, norm):
        if norm.weight is not None:
            nn.init.constant_(norm.weight, 1)
            nn.init.constant_(norm.bias, 0)


class IBNorm(nn.Module):
    def __init__(self, num_features, split=0.5):
        super(IBNorm, self).__init__()

        self.num_features = num_features
        self.num_BN = int(num_features * split + 0.5)
        self.bnorm = nn.BatchNorm2d(num_features=self.num_BN, affine=True)
        self.inorm = nn.InstanceNorm2d(num_features=num_features - self.num_BN, affine=False)

    def forward(self, x):
        if self.num_BN == self.num_features:
            return self.bnorm(x.contiguous())
        else:
            xb = self.bnorm(x[:, 0:self.num_BN, :, :].contiguous())
            xi = self.inorm(x[:, self.num_BN:, :, :].contiguous())

            return torch.cat((xb, xi), 1)


class EncoderBlock(nn.Module):
    def __init__(self, in_channel, out_channel, stride=1):
        super(EncoderBlock, self).__init__()

        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, bias=False)
        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)

        self.bn1 = IBNorm(out_channel)
        self.bn2 = nn.BatchNorm2d(out_channel)

        self.relu = nn.ReLU(inplace=True)
        self.pooling = nn.AvgPool2d(stride)

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.pooling(x)
        return x


class DecoderBlock(nn.Module):
    def __init__(self, in_channel, out_channel):
        super(DecoderBlock, self).__init__()

        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, bias=False)
        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)

        self.bn1 = nn.BatchNorm2d(out_channel)
        self.bn2 = nn.BatchNorm2d(out_channel)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x, y=None):
        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
        if y is not None:
            x = torch.cat((x, y), 1)

        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        return x


class AbsoluteCriterion(nn.Module):
    """ Absolute loss function.

    The formula is:
        loss = sqrt((pred - x)^2 + 1e-6)
    """

    def __init__(self):
        super(AbsoluteCriterion, self).__init__()

    def forward(self, pred, gt):
        assert not gt.requires_grad
        if not len(pred.shape) == len(gt.shape) == 4:
            print('AbsoluteCriterion reaqires 4-dims tensor as inputs, '
                    'however it gets {0}-dims (pred) and {1}-dims (gt)\n'
                    .format(len(pred.shape), len(gt.shape)))
            exit()
        loss = torch.sqrt((pred - gt) ** 2 + 1e-6)
        return loss


class GradientCriterion(nn.Module):
    """ Gradient loss function.

    This gradient loss function only handle x and y directions.
    The formula is:
        loss = ( (pred_grad_x^2 + pred_grad_y^2 + 1e-6) - (gt_grad_x^2 + gt_grad_y^2 + 1e-6) )
    """
    
    def __init__(self):
        super(GradientCriterion, self).__init__()

        x_kernel = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]]) / 8
        self.conv_x = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)
        self.conv_x.weight.data = torch.tensor(x_kernel).unsqueeze(0).unsqueeze(0).float().cuda()
        self.conv_x.weight.requires_grad = False

        y_kernel = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]) / 8
        self.conv_y = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)
        self.conv_y.weight.data = torch.tensor(y_kernel).unsqueeze(0).unsqueeze(0).float().cuda()
        self.conv_y.weight.requires_grad = False

    def forward(self, pred, gt):
        assert not gt.requires_grad
        if not len(pred.shape) == len(gt.shape) == 4:
            print('GradientCriterion reaqires 4-dims tensor as inputs, '
                    'however it gets {0}-dims (pred) and {1}-dims (gt)\n'
                    .format(len(pred.shape), len(gt.shape)))
            exit()

        pred_grad_x, pred_grad_y = self.conv_x(pred), self.conv_y(pred)
        pred_grad = torch.sqrt(pred_grad_x ** 2 + pred_grad_y ** 2 + 1e-6)

        gt_grad_x, gt_grad_y = self.conv_x(gt), self.conv_y(gt)
        gt_grad = torch.sqrt(gt_grad_x ** 2 + gt_grad_y ** 2 + 1e-6)

        loss = torch.abs(pred_grad - gt_grad)
        return loss
        